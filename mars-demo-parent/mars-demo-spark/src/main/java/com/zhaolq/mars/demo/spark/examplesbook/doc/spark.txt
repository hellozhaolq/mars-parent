Spark是一个软框架，管理和协调跨多台计算机的计算任务。

Spark有两套基本的API，低级非结构化API和更高级别的结构化API。

DataFrame是最常见的结构化API，是一个包含行和列的数据表，说明这些列和列类型的一些规则被称为模式（schema）。跨越数千台计算机存储。

Spark几个核心抽象：Dataset，DataFrame，SQL表，弹性分布式数据集（Resilient Distributed Datasets，RDD）。最简单最有效的是DataFrame。

数据分区：当使用DataFrame时，不需要手动操作分区，只需指定数据的高级转换操作，然后Spark决定此工作如何在集群上执行。较低级别的API（RDD）也是如此。

一个Spark作业包含一系列转换操作并由一个动作操作触发，并可以通过Spark UI监视该作业
转换操作：在调用一个动作操作之前，Spark不会真正的执行转换操作。它是使用Spark表达业务逻辑的核心，有两类：
	窄依赖关系：每个输入分区仅决定一个输出分区的转换，一对一映射。在内存中执行。属于窄转换操作的有：过滤
	宽依赖关系：每个输入分区决定了多个输出分区的转换。又叫洗牌操作，它会在整个集群中执行互相交换分区数据的功能。该操作会将结果写入磁盘。属于宽转换操作的有：聚合
	只需要了解存在这两种转换形式。
	惰性评估：等到绝对需要时才执行。在Spark中，当用户表达一些对数据的操作时，不是立即修改数据，而是建立一个作用到原始数据的转换计划。Spark首先会将这个计划编译为可以在集群中高效运行的流水线式的物理执行计划，然后等待，直到最后时刻才开始执行代码。这样有很多好处，Spark可以优化整个从输入端到输出端的数据流，例如Spark会通过自动下推过滤操作来优化整个物理执行计划（谓词下推）。
动作操作：为了出发计算，需要运行一个动作操作（action）。有三类动作：
	在控制台中查看数据的动作。
	在某个语言中将数据汇集为原生对象的动作。如：collect操作将所有分区的结果汇集到一起生成某个语言的一个原生对象。
	写入输出数据源的动作。







